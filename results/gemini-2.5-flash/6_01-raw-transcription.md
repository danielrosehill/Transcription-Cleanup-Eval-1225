# GEMINI-2.5-FLASH - 6 - 01-raw-transcription

**Audio:** 6.mp3
**Prompt:** 01-raw-transcription
**Model:** gemini-2.5-flash

---

00:00 Okay, so here's the purpose of this repository and I'll also just record this for the sake of 00:05 and in context data but it's going to be itself used in this evaluation. 00:10 The 00:13 I've been doing a lot of um using transcription tools pretty much every day for the past year. 00:18 And um feel like I'm 00:20 getting 00:22 close to having sort of a 00:25 decisive set of tools that I can use on different 00:29 environments for really using these to the as well as they can be used. 00:33 Um something that I believe has enormous promise is audio multi-modal 00:37 which is multimodal AI models that can take audio as a binary and they can produce text 00:43 that is a little bit formatted for specific purposes. 00:48 Uh the difference is compared to traditional ASR models where you get a verbatim transcript and then you can add a prompt like clean it up a bit. 00:56 remove the filler words, remove the pauses or reformat that as an email. 01:00 And why I'm very interested in exploring these models is that you can consolidate that to one step 01:05 and that just makes it much more efficient and uh and easy to use, I would say. 01:10 So, 01:13 what I want to um 01:16 do in this, the reason I've created this little evaluation, and I emphasize that it's just kind of a gaining a 01:20 I want to quickly test out the different models. I created a repo a few days ago 01:25 in which I um just went on on uh hugging face and clearly 01:31 um 01:33 clearly Gemini and open it. So Gemini and OpenAI are the ones I've been using for this 01:37 mostly to date and the Gemini tool is brilliant. 01:41 But I thought 01:43 is there any chance that this could be done locally? So to answer that question, I went looking for open source 01:49 um models that have the audio modality. 01:52 And I found on hugging face, there is an audio text to text task, 01:56 and there is an omnitask. Omni is any to any. 02:00 But within the first level, um, I created 02:03 a repository just to kind of say at this point in time, here's what there is. 02:07 And I the only one I've tried so far locally is actually Foxtrot. 02:12 Um, and it basically would work but wouldn't work in the sense that 02:16 I validated that it it did work, it ran on my hardware. 02:20 Um but it was 02:22 throttling the GPU like crazy. It was using almost all of my GPU. So it's actually it's one that I can confirm is viable. I was able to achieve inference. 02:30 And the prompt worked, but um at the same time 02:34 for so it's which is strange because it's 9B, it didn't seem like it should 02:39 be so difficult, but I'm looking at the list now there's also Five multimodal to try out. It's only 2.6B and there is 02:47 Let's see what else. 02:50 Omni audio as well might be worth trying out because it's got quite a small uh quantize version. Maybe it's just inherently that the this task 02:57 of audio understanding requires larger models than ASR and and there's nothing to be done about it. 03:03 Um 03:05 The purpose of this particular evaluation is to 03:09 run try out these different models for this task of audio understanding in the 03:15 narrow context that I'm considering it within, which is that of 03:19 what you might I don't know if there's a word for this transcribe and rewrite, maybe it's just called transcribe. 03:24 Something to note is that sometimes these model providers actually 03:28 offer a transcription end point for the multimodals. 03:32 And then if you want to do the transcribe rewrite workflow, which the workflow, which might be 03:37 just cleanup or reformat this as an email or reformat this as a task list in JSON, which is what I find which is the one that I'm very excited and interested about. 03:47 You actually need to use the chat API endpoint, which to me doesn't actually make a whole bunch of sense, but that's the way 03:54 at least Mistral have 03:57 done it. Um so 04:00 here's what I want to do in this evaluation. Um the plan the plan of action. 04:05 Gemini and OpenAI 04:09 I know. 04:10 familiar with but we may as well add them. Now all these other ones 04:16 Mistral. 04:17 Flamingo, Quen, 04:20 The question Quen 2, I should say and Quen Omni. The challenge is usually finding inference 04:26 providers in the cloud because um most of them I can't run locally 04:31 most of them are non-starters. 04:33 Um but places like Fireworks there are some providers that do them. So this isn't going to be like a big 04:40 evaluation at all. It's going to be just testing is Quen 3 any good basically for this. 04:46 And what I'm going to do is just I've created six, this will be the sixth and final audio sample that I'm recording. 04:53 And the previous five are kind of very much reflective of the type of workflow that I'd like to try out here, which is 05:00 I'm recording something, it's a it's an instruction for an AI agent, it's a development prompt, it's an email. I don't do not want a verbatim transcript, but 05:10 we might add 05:12 we might add for the sake of completion. We might ask them to do that anyway, just to see how they fair in that task. 05:18 Um but the real one I'm evaluating on is 05:21 the cleaned up, well there's two levels really. The first one is a cleanup. 05:26 Remove the ems, the ums, the pauses. I'd like to try as well their ability to handle inferred instructions, which is when I say something like 05:35 um I want to go out today at 6:00, oh wait, sorry, I meant 5:00. And 05:41 it'd be very interesting in this audio multimodal uh evaluation, I might create one more sample just with 05:47 a couple of those to see how they handle that um that task, which ones actually can infer can 05:54 can uh process that as an instruction and not as 05:58 and transcribe accordingly. 06:00 And finally, 06:02 I'd like to try one prompt for formatted here and which is saying, 06:06 this prompt for example, um, I think it's number five, is a development instruction prompt intended for 06:14 to be provided to an AI agent tool for making some edits to the user's personal website and edit that accordingly. And that's a that's a prompt to test their ability to generate 06:25 um text to a specific format in response to the audio input.